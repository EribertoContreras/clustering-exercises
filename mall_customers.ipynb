{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fbc34fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wrangle_zillow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dc/8rtfvpyj2mb6mqbnfkf5dl400000gn/T/ipykernel_39997/2228206835.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Imports functions necessary to run visuals and hides unnecessary code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwrangle_zillow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# coding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wrangle_zillow'"
     ]
    }
   ],
   "source": [
    "# Fetches the data\n",
    "import acquire\n",
    "# credentials file to access the data\n",
    "import env\n",
    "# Imports functions necessary to run visuals and hides unnecessary code\n",
    "import wrangle_zillow\n",
    "\n",
    "# coding \n",
    "import math\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pydataset import data\n",
    "import scipy.stats\n",
    "import scipy\n",
    "import os\n",
    "\n",
    "# needed for modeling\n",
    "import sklearn.preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, QuantileTransformer\n",
    "from sklearn.metrics import explained_variance_score\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression, LassoLars, TweedieRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22eafec",
   "metadata": {},
   "source": [
    "### Mall Customers\n",
    "    Notebook\n",
    "\n",
    "1) Acquire data from the customers table in the mall_customers database.\n",
    "\n",
    "2) Summarize the data (include distributions and descriptive statistics).\n",
    "\n",
    "3) Detect outliers using IQR.\n",
    "\n",
    "4) Split data into train, validate, and test.\n",
    "\n",
    "5) Encode categorical columns using a one hot encoder (pd.get_dummies).\n",
    "\n",
    "6) Handles missing values.\n",
    "\n",
    "7) Scaling\n",
    "\n",
    "    Encapsulate your work in a wrangle_mall.py python module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8c427a",
   "metadata": {},
   "source": [
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747d3848",
   "metadata": {},
   "source": [
    "1) Acquire data from the customers table in the mall_customers database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a26eb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquire():\n",
    "    database = 'mall_customers'\n",
    "    url = f'mysql+pymysql://{env.user}:{env.password}@{env.host}/{database}'\n",
    "    df = pd.read_sql('SELECT * FROM customers', url, index_col='customer_id')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb8f403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acquire()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef4f9c4",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a19cbaf",
   "metadata": {},
   "source": [
    "2) Summarize the data (include distributions and descriptive statistics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d184b26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Shape: (200, 4)\n",
      "--- Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 200 entries, 1 to 200\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   gender          200 non-null    object\n",
      " 1   age             200 non-null    int64 \n",
      " 2   annual_income   200 non-null    int64 \n",
      " 3   spending_score  200 non-null    int64 \n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 7.8+ KB\n",
      "None\n",
      "--- Descriptions:               age  annual_income  spending_score\n",
      "count  200.000000     200.000000      200.000000\n",
      "mean    38.850000      60.560000       50.200000\n",
      "std     13.969007      26.264721       25.823522\n",
      "min     18.000000      15.000000        1.000000\n",
      "25%     28.750000      41.500000       34.750000\n",
      "50%     36.000000      61.500000       50.000000\n",
      "75%     49.000000      78.000000       73.000000\n",
      "max     70.000000     137.000000       99.000000\n",
      "--- Nulls by Column: gender            0\n",
      "age               0\n",
      "annual_income     0\n",
      "spending_score    0\n",
      "dtype: int64\n",
      "nulls by row: n_missing  percent_missing\n",
      "0          0.0                200\n",
      "dtype: int64\n",
      "None None None None None None\n"
     ]
    }
   ],
   "source": [
    "def summarize(df):\n",
    "    \"combines these functions to summarize the data set given to us\"\n",
    "    a = print('--- Shape: {}'.format(df.shape))\n",
    "    #returns the information from the data\n",
    "    bb = print('--- Info:')\n",
    "    b = print((df.info()))\n",
    "     # describes the data\n",
    "    c = print('--- Descriptions:', (df.describe()))\n",
    "    # returns the sum of null values in columns\n",
    "    d = print('--- Nulls by Column:', (df.isnull().sum()))\n",
    "    # returns nulls by row\n",
    "    e = print('nulls by row:', (pd.concat([df.isna().sum(axis=1).rename('n_missing'),df.isna().mean(axis=1).rename('percent_missing'),], axis=1).value_counts().sort_index()))\n",
    "    print(a, bb,b, c, d, e)\n",
    "summarize(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edfaeac",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83192ce6",
   "metadata": {},
   "source": [
    "3) Detect outliers using IQR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89c2c90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annual_income</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spending_score</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                count  percent\n",
       "gender              0      0.0\n",
       "age                 0      0.0\n",
       "annual_income       0      0.0\n",
       "spending_score      0      0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nulls_by_columns(df):\n",
    "    # gives us a count and a percent of missing information.\n",
    "    return pd.concat([\n",
    "        df.isna().sum().rename('count'),\n",
    "        df.isna().mean().rename('percent')\n",
    "    ], axis=1)\n",
    "nulls_by_columns(df).sort_values(by= 'percent', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "356974cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_function(df, cols, k):\n",
    "    for col in df[cols]:\n",
    "        q1 = df[col].quantile(0.25)\n",
    "        q3 = df[col].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        upper_bound = q3 + k * iqr\n",
    "        lower_bound = q1 - k * iqr\n",
    "        df = df[(df[col] < upper_bound) & (df[col] > lower_bound)]\n",
    "        return df\n",
    "\n",
    "def handle_missing_value(df, prop_required_column, prop_required_row):\n",
    "    #this piece of code allows us to handle the missing data and get rid of it, both in the columns and in the rows(so that we can analize better).\n",
    "    n_required_column = round(df.shape[0] * prop_required_column)\n",
    "    n_required_row = round(df.shape[1] * prop_required_row)\n",
    "    df = df.dropna(axis=0, thresh=n_required_row)\n",
    "    df = df.dropna(axis=1, thresh=n_required_column)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c51847f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outlier_function(df, df.columns, k = 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bffa519",
   "metadata": {},
   "source": [
    "4) Split data into train, validate, and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d81e6422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exploration_data():\n",
    "    df = acquire()\n",
    "    print ('Before dropping nulls, %d rows, %d cols' % df.shape)\n",
    "    df = handle_missing_values(af, prop_required_column=.5, prop_required_row=.5)\n",
    "    print('After dropping nulls. %d rows. %d cols' % df.shape)\n",
    "    train, validate, test = split(df)\n",
    "    return train\n",
    "##############################################################################################\n",
    "def get_modeling_data(scale_data=False):\n",
    "    df = acquire()\n",
    "    print('Before dropping nulls, %d rows, %d cols' % df.shape)\n",
    "    df = handle_missing_values(df, prop_required_column=.5, prop_required_row=.5)\n",
    "    print('After dropping nulls, sd rows, %d cols' % df.shape)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    print('Before removing outliers, %d rows, %d cols' % df. shape)\n",
    "    outlier_function(df, ['age','spending_score','annual_income'], 1.5)\n",
    "    print('after dropping nulls, %d rows, %d cols' % df.shape)\n",
    "    print()\n",
    "    \n",
    "    df = one_hot_encode(df)\n",
    "    \n",
    "    train, validate, test = split(df)\n",
    "    if scale_data:\n",
    "        return scale(train, validate, test)\n",
    "    else:\n",
    "        train, validate, test\n",
    "##############################################################################################\n",
    "def split(df):\n",
    "    train_and_validate, test = train_test_split(df, random_state=13, test_size=.15)\n",
    "    train, validate = train_test_split(train and validate, random_state=13, test_size=.2)\n",
    "    print('Train: %d rows, %d cols' % train.shape)\n",
    "    print ('Validate: %d rows, %d cols' % validate. shape)\n",
    "    print ('Test: %d rows, %d cols' % test.shape)\n",
    "    \n",
    "    return train, validate, test\n",
    "##############################################################################################\n",
    "def scale(train, validate, test):\n",
    "    columns_to_scale = ['age','spending_score', 'annual_income']\n",
    "    train_scaled = train.copy()\n",
    "    validate_scaled = validate.copy()\n",
    "    test_scaled = test.copy()\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(train[columns_to_scale])\n",
    "    train_scaled[columns_to_scale] = scaler.transform(train[columns_to_scale])\n",
    "    validate_scaled[columns_to_scale] = scaler.transform(validate[columns_to_scale])\n",
    "    test_scaled[columns_to_scale] = scaler.transform(test[columns_to_scale])\n",
    "    return scaler, train_scaled, validate_scaled, test_scaled\n",
    "##############################################################################################\n",
    "def one_hot_encode(df):\n",
    "    df['is_female'] = df.gender == 'Female'\n",
    "    df = df.drop(columns='gender')\n",
    "    return df\n",
    "##############################################################################################\n",
    "def handle_missing_values(df, prop_required_column, prop_required_row):\n",
    "    n_required_column = round(df.shape[0] * prop_required_column)\n",
    "    n_required_row = round(df.shape[1] * prop_required_row)\n",
    "    df = df.dropna(axis=0, thresh=n_required_row)\n",
    "    df = df.dropna(axis=1, thresh=n_required_column)\n",
    "    return df\n",
    "##############################################################################################\n",
    "def handle_outliers(df, cols, k):\n",
    "    # Create placeholder dictionary for each columns bounds\n",
    "    bounds_dict = {}\n",
    "\n",
    "    # get a list of all columns that are not object type\n",
    "    non_object_cols = df.dtypes[df.dtypes != 'object'].index\n",
    "\n",
    "\n",
    "    for col in non_object_cols:\n",
    "        # get necessary iqr values\n",
    "        q1 = df[col].quantile(0.25)\n",
    "        q3 = df[col].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        upper_bound =  q3 + k * iqr\n",
    "        lower_bound =  q1 - k * iqr\n",
    "\n",
    "        #store values in a dictionary referencable by the column name\n",
    "        #and specific bound\n",
    "        bounds_dict[col] = {}\n",
    "        bounds_dict[col]['upper_bound'] = upper_bound\n",
    "        bounds_dict[col]['lower_bound'] = lower_bound\n",
    "\n",
    "    for col in non_object_cols:\n",
    "        #retrieve bounds\n",
    "        col_upper_bound = bounds_dict[col]['upper_bound']\n",
    "        col_lower_bound = bounds_dict[col]['lower_bound']\n",
    "\n",
    "        #remove rows with an outlier in that column\n",
    "        df = df[(df[col] < col_upper_bound) & (df[col] > col_lower_bound)]\n",
    "    \n",
    "    return df\n",
    "##############################################################################################\n",
    "def split(df):\n",
    "    train_and_validate, test = train_test_split(df, random_state=13, test_size=.15)\n",
    "    train, validate = train_test_split(train_and_validate, random_state=13, test_size=.2)\n",
    "\n",
    "    print('Train: %d rows, %d cols' % train.shape)\n",
    "    print('Validate: %d rows, %d cols' % validate.shape)\n",
    "    print('Test: %d rows, %d cols' % test.shape)\n",
    "\n",
    "    return train, validate, test\n",
    "##############################################################################################\n",
    "def scale(train, validate, test):\n",
    "    columns_to_scale = [] #define columns and input here.\n",
    "    train_scaled = train.copy()\n",
    "    validate_scaled = validate.copy()\n",
    "    test_scaled = test.copy()\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(train[columns_to_scale])\n",
    "\n",
    "    train_scaled[columns_to_scale] = scaler.transform(train[columns_to_scale])\n",
    "    validate_scaled[columns_to_scale] = scaler.transform(validate[columns_to_scale])\n",
    "    test_scaled[columns_to_scale] = scaler.transform(test[columns_to_scale])\n",
    "\n",
    "    return scaler, train_scaled, validate_scaled, test_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de4d0501",
   "metadata": {},
   "outputs": [],
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a629c65f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
